################################################################################
# PkgEval set-up: 2021-01-11T15:25:24.3
#

Julia Version 1.5.3
Commit 599f52c4c6 (2020-12-18 13:33 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: AMD Ryzen Threadripper 2990WX 32-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, znver1)
Environment:
  JULIA_PKG_PRECOMPILE_AUTO = 0
  JULIA_PKGEVAL = true

Base.julia_cmd() = `/opt/julia/bin/julia -Cnative -J/opt/julia/lib/julia/sys.so -g1`


################################################################################
# Installation: 2021-01-11T15:25:25.937
#

 Installing known registries into `~/.julia`
#=#=#                                                                         ###############################                                           44.1%######################################################################## 100.0%
      Added registry `General` to `~/.julia/registries/General`
  Resolving package versions...
  Installed ChainRules ─────────────────── v0.7.45
  Installed NonlinearSolve ─────────────── v0.3.4
  Installed LoggingExtras ──────────────── v0.4.2
  Installed Zlib_jll ───────────────────── v1.2.11+18
  Installed NNlib ──────────────────────── v0.7.11
  Installed DiffEqFlux ─────────────────── v1.30.0
  Installed DataAPI ────────────────────── v1.4.0
  Installed Compat ─────────────────────── v3.25.0
  Installed GPUCompiler ────────────────── v0.8.3
  Installed DiffEqBase ─────────────────── v6.53.4
  Installed DiffEqSensitivity ──────────── v6.35.0
  Installed OrdinaryDiffEq ─────────────── v5.49.1
  Installed MacroTools ─────────────────── v0.5.6
  Installed Juno ───────────────────────── v0.8.4
  Installed TreeViews ──────────────────── v0.3.0
  Installed SLEEFPirates ───────────────── v0.5.5
  Installed Zygote ─────────────────────── v0.5.17
  Installed MKL_jll ────────────────────── v2020.2.254+0
  Installed DistributionsAD ────────────── v0.6.15
  Installed CpuId ──────────────────────── v0.2.2
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed PoissonRandom ──────────────── v0.4.0
  Installed RecursiveArrayTools ────────── v2.10.0
  Installed Missings ───────────────────── v0.4.4
  Installed Optim ──────────────────────── v1.2.2
  Installed FixedPointNumbers ──────────── v0.8.4
  Installed LightGraphs ────────────────── v1.3.4
  Installed BFloat16s ──────────────────── v0.1.0
  Installed ReverseDiff ────────────────── v1.5.0
  Installed Media ──────────────────────── v0.5.0
  Installed DataValueInterfaces ────────── v1.0.0
  Installed Random123 ──────────────────── v1.2.0
  Installed DiffRules ──────────────────── v1.0.2
  Installed MuladdMacro ────────────────── v0.2.2
  Installed LoopVectorization ──────────── v0.8.26
  Installed LLVM ───────────────────────── v3.5.2
  Installed GalacticOptim ──────────────── v0.4.4
  Installed DiffEqJump ─────────────────── v6.12.2
  Installed Artifacts ──────────────────── v1.3.0
  Installed ProgressMeter ──────────────── v1.4.1
  Installed FFTW_jll ───────────────────── v3.3.9+7
  Installed Distances ──────────────────── v0.10.0
  Installed ForwardDiff ────────────────── v0.10.14
  Installed Rmath ──────────────────────── v0.6.1
  Installed ColorTypes ─────────────────── v0.10.9
  Installed CodecZlib ──────────────────── v0.7.0
  Installed Adapt ──────────────────────── v2.4.0
  Installed DiffEqCallbacks ────────────── v2.16.0
  Installed VectorizationBase ──────────── v0.12.33
  Installed DocStringExtensions ────────── v0.8.3
  Installed RecipesBase ────────────────── v1.1.1
  Installed CUDA ───────────────────────── v2.4.0
  Installed ExprTools ──────────────────── v0.1.3
  Installed ArrayLayouts ───────────────── v0.4.11
  Installed ConsoleProgressMonitor ─────── v0.1.2
  Installed Colors ─────────────────────── v0.12.6
  Installed PositiveFactorizations ─────── v0.2.4
  Installed Requires ───────────────────── v1.1.2
  Installed OpenSpecFun_jll ────────────── v0.5.3+4
  Installed DiffEqNoiseProcess ─────────── v5.5.0
  Installed Functors ───────────────────── v0.1.0
  Installed JLLWrappers ────────────────── v1.2.0
  Installed PDMats ─────────────────────── v0.10.1
  Installed ProgressLogging ────────────── v0.1.3
  Installed AbstractTrees ──────────────── v0.3.3
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed StaticArrays ───────────────── v0.12.5
  Installed TableTraits ────────────────── v1.0.0
  Installed FunctionWrappers ───────────── v1.1.1
  Installed LeftChildRightSiblingTrees ─── v0.1.2
  Installed IteratorInterfaceExtensions ── v1.0.0
  Installed ZipFile ────────────────────── v0.9.3
  Installed GenericSVD ─────────────────── v0.3.0
  Installed LatticeRules ───────────────── v0.0.1
  Installed NaNMath ────────────────────── v0.3.5
  Installed SimpleTraits ───────────────── v0.9.3
  Installed FillArrays ─────────────────── v0.9.7
  Installed VertexSafeGraphs ───────────── v0.1.2
  Installed ChainRulesCore ─────────────── v0.9.24
  Installed TranscodingStreams ─────────── v0.9.5
  Installed TerminalLoggers ────────────── v0.1.2
  Installed ZygoteRules ────────────────── v0.2.1
  Installed ConstructionBase ───────────── v1.0.0
  Installed QuadGK ─────────────────────── v2.4.1
  Installed Flux ───────────────────────── v0.11.3
  Installed Distributions ──────────────── v0.24.10
  Installed IterativeSolvers ───────────── v0.9.0
  Installed Scratch ────────────────────── v1.0.3
  Installed OffsetArrays ───────────────── v1.5.0
  Installed RandomNumbers ──────────────── v1.4.0
  Installed QuasiMonteCarlo ────────────── v0.2.2
  Installed SpecialFunctions ───────────── v1.2.1
  Installed ExponentialUtilities ───────── v1.8.0
  Installed Tables ─────────────────────── v1.2.2
  Installed UnPack ─────────────────────── v1.0.2
  Installed Rmath_jll ──────────────────── v0.2.2+1
  Installed SparseDiffTools ────────────── v1.12.0
  Installed FastClosures ───────────────── v0.3.2
  Installed GPUArrays ──────────────────── v6.2.0
  Installed Inflate ────────────────────── v0.1.2
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed LineSearches ───────────────── v7.1.1
  Installed OrderedCollections ─────────── v1.3.2
  Installed NLSolversBase ──────────────── v7.7.1
  Installed RecursiveFactorization ─────── v0.1.6
  Installed Setfield ───────────────────── v0.7.0
  Installed FiniteDiff ─────────────────── v2.7.2
  Installed CommonSubexpressions ───────── v0.3.0
  Installed DataStructures ─────────────── v0.18.8
  Installed LatinHypercubeSampling ─────── v1.7.3
  Installed LabelledArrays ─────────────── v1.4.0
  Installed StableRNGs ─────────────────── v1.0.0
  Installed NLsolve ────────────────────── v4.5.1
  Installed StatsFuns ──────────────────── v0.9.6
  Installed DiffResults ────────────────── v1.0.3
  Installed Parameters ─────────────────── v0.12.1
  Installed Tracker ────────────────────── v0.2.14
  Installed TimerOutputs ───────────────── v0.5.7
  Installed Reexport ───────────────────── v0.2.0
  Installed SIMDPirates ────────────────── v0.8.26
  Installed ArrayInterface ─────────────── v2.14.13
  Installed StochasticDiffEq ───────────── v6.30.1
  Installed FFTW ───────────────────────── v1.3.0
  Installed IRTools ────────────────────── v0.4.2
  Installed ArnoldiMethod ──────────────── v0.0.4
  Installed ResettableStacks ───────────── v1.1.0
  Installed CEnum ──────────────────────── v0.4.1
  Installed StatsBase ──────────────────── v0.33.2
  Installed CompilerSupportLibraries_jll ─ v0.3.4+0
  Installed Sobol ──────────────────────── v1.4.0
Updating `~/.julia/environments/v1.5/Project.toml`
  [aae7a2af] + DiffEqFlux v1.30.0
Updating `~/.julia/environments/v1.5/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v2.4.0
  [ec485272] + ArnoldiMethod v0.0.4
  [4fba245c] + ArrayInterface v2.14.13
  [4c555306] + ArrayLayouts v0.4.11
  [56f22d72] + Artifacts v1.3.0
  [ab4f0b2a] + BFloat16s v0.1.0
  [fa961155] + CEnum v0.4.1
  [052768ef] + CUDA v2.4.0
  [082447d4] + ChainRules v0.7.45
  [d360d2e6] + ChainRulesCore v0.9.24
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.9
  [5ae59095] + Colors v0.12.6
  [bbf7d656] + CommonSubexpressions v0.3.0
  [34da2185] + Compat v3.25.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.4+0
  [88cd18e8] + ConsoleProgressMonitor v0.1.2
  [187b0558] + ConstructionBase v1.0.0
  [adafc99b] + CpuId v0.2.2
  [9a962f9c] + DataAPI v1.4.0
  [864edb3b] + DataStructures v0.18.8
  [e2d170a0] + DataValueInterfaces v1.0.0
  [2b5f629d] + DiffEqBase v6.53.4
  [459566f4] + DiffEqCallbacks v2.16.0
  [aae7a2af] + DiffEqFlux v1.30.0
  [c894b116] + DiffEqJump v6.12.2
  [77a26b50] + DiffEqNoiseProcess v5.5.0
  [41bf760c] + DiffEqSensitivity v6.35.0
  [163ba53b] + DiffResults v1.0.3
  [b552c78f] + DiffRules v1.0.2
  [b4f34e82] + Distances v0.10.0
  [31c24e10] + Distributions v0.24.10
  [ced4e74d] + DistributionsAD v0.6.15
  [ffbed154] + DocStringExtensions v0.8.3
  [d4d017d3] + ExponentialUtilities v1.8.0
  [e2ba6199] + ExprTools v0.1.3
  [7a1cc6ca] + FFTW v1.3.0
  [f5851436] + FFTW_jll v3.3.9+7
  [9aa1b823] + FastClosures v0.3.2
  [1a297f60] + FillArrays v0.9.7
  [6a86dc24] + FiniteDiff v2.7.2
  [53c48c17] + FixedPointNumbers v0.8.4
  [587475ba] + Flux v0.11.3
  [f6369f11] + ForwardDiff v0.10.14
  [069b7b12] + FunctionWrappers v1.1.1
  [d9f16b24] + Functors v0.1.0
  [0c68f7d7] + GPUArrays v6.2.0
  [61eb1bfa] + GPUCompiler v0.8.3
  [a75be94c] + GalacticOptim v0.4.4
  [01680d73] + GenericSVD v0.3.0
  [7869d1d1] + IRTools v0.4.2
  [d25df0c9] + Inflate v0.1.2
  [1d5cc7b8] + IntelOpenMP_jll v2018.0.3+0
  [42fd0dbc] + IterativeSolvers v0.9.0
  [82899510] + IteratorInterfaceExtensions v1.0.0
  [692b3bcd] + JLLWrappers v1.2.0
  [e5e0dc1b] + Juno v0.8.4
  [929cbde3] + LLVM v3.5.2
  [2ee39098] + LabelledArrays v1.4.0
  [a5e1c1ea] + LatinHypercubeSampling v1.7.3
  [73f95e8e] + LatticeRules v0.0.1
  [1d6d02ad] + LeftChildRightSiblingTrees v0.1.2
  [093fc24a] + LightGraphs v1.3.4
  [d3d80556] + LineSearches v7.1.1
  [e6f89c97] + LoggingExtras v0.4.2
  [bdcacae8] + LoopVectorization v0.8.26
  [856f044c] + MKL_jll v2020.2.254+0
  [1914dd2f] + MacroTools v0.5.6
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.4
  [46d2c3a1] + MuladdMacro v0.2.2
  [d41bc354] + NLSolversBase v7.7.1
  [2774e3e8] + NLsolve v4.5.1
  [872c559c] + NNlib v0.7.11
  [77ba4419] + NaNMath v0.3.5
  [8913a72c] + NonlinearSolve v0.3.4
  [6fe1bfb0] + OffsetArrays v1.5.0
  [efe28fd5] + OpenSpecFun_jll v0.5.3+4
  [429524aa] + Optim v1.2.2
  [bac558e1] + OrderedCollections v1.3.2
  [1dea7af3] + OrdinaryDiffEq v5.49.1
  [90014a1f] + PDMats v0.10.1
  [d96e819e] + Parameters v0.12.1
  [e409e4f3] + PoissonRandom v0.4.0
  [85a6dd25] + PositiveFactorizations v0.2.4
  [33c8b6b6] + ProgressLogging v0.1.3
  [92933f4c] + ProgressMeter v1.4.1
  [1fd47b50] + QuadGK v2.4.1
  [8a4e6c94] + QuasiMonteCarlo v0.2.2
  [74087812] + Random123 v1.2.0
  [e6cf234a] + RandomNumbers v1.4.0
  [3cdcf5f2] + RecipesBase v1.1.1
  [731186ca] + RecursiveArrayTools v2.10.0
  [f2c3362d] + RecursiveFactorization v0.1.6
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.1.2
  [ae5879a3] + ResettableStacks v1.1.0
  [37e2e3b7] + ReverseDiff v1.5.0
  [79098fc4] + Rmath v0.6.1
  [f50d1b31] + Rmath_jll v0.2.2+1
  [21efa798] + SIMDPirates v0.8.26
  [476501e8] + SLEEFPirates v0.5.5
  [6c6a2e73] + Scratch v1.0.3
  [efcf1570] + Setfield v0.7.0
  [699a6c99] + SimpleTraits v0.9.3
  [ed01d8cd] + Sobol v1.4.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [47a9eef4] + SparseDiffTools v1.12.0
  [276daf66] + SpecialFunctions v1.2.1
  [860ef19b] + StableRNGs v1.0.0
  [90137ffa] + StaticArrays v0.12.5
  [2913bbd2] + StatsBase v0.33.2
  [4c63d2b9] + StatsFuns v0.9.6
  [789caeaf] + StochasticDiffEq v6.30.1
  [3783bdb8] + TableTraits v1.0.0
  [bd369af6] + Tables v1.2.2
  [5d786b92] + TerminalLoggers v0.1.2
  [a759f4b9] + TimerOutputs v0.5.7
  [9f7883ad] + Tracker v0.2.14
  [3bb67fe8] + TranscodingStreams v0.9.5
  [a2a6695c] + TreeViews v0.3.0
  [3a884ed6] + UnPack v1.0.2
  [3d5dd08c] + VectorizationBase v0.12.33
  [19fa3120] + VertexSafeGraphs v0.1.2
  [a5390f91] + ZipFile v0.9.3
  [83775a58] + Zlib_jll v1.2.11+18
  [e88e6eb3] + Zygote v0.5.17
  [700de1a5] + ZygoteRules v0.2.1
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8bb1440f] + DelimitedFiles
  [8ba89e20] + Distributed
  [9fa8497b] + Future
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [9abbd945] + Profile
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [1a1011a3] + SharedArrays
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [4607b0f0] + SuiteSparse
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building SLEEFPirates → `~/.julia/packages/SLEEFPirates/jGsib/deps/build.log`
   Building Random123 ───→ `~/.julia/packages/Random123/Y2Du3/deps/build.log`
   Building FFTW ────────→ `~/.julia/packages/FFTW/ayqyZ/deps/build.log`


################################################################################
# Testing: 2021-01-11T15:25:46.457
#

    Testing DiffEqFlux
┌ Error: Pkg.Resolve.ResolverError("Unsatisfiable requirements detected for package GeometricFlux [7e08b658]:\n GeometricFlux [7e08b658] log:\n ├─possible versions are: [0.1.0-0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.5.0-0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5] or uninstalled\n ├─restricted to versions * by an explicit requirement, leaving only versions [0.1.0-0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.5.0-0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5]\n ├─restricted by compatibility requirements with Zygote [e88e6eb3] to versions: [0.1.0-0.1.1, 0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5] or uninstalled, leaving only versions: [0.1.0-0.1.1, 0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5]\n │ └─Zygote [e88e6eb3] log:\n │   ├─possible versions are: [0.1.0, 0.2.0, 0.3.0-0.3.4, 0.4.0-0.4.22, 0.5.0-0.5.17, 0.6.0] or uninstalled\n │   └─restricted to versions 0.5.17 by an explicit requirement, leaving only versions 0.5.17\n ├─restricted by compatibility requirements with Requires [ae029012] to versions: [0.3.0, 0.4.0, 0.5.0-0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5] or uninstalled, leaving only versions: [0.5.2, 0.6.0-0.6.3, 0.7.0-0.7.5]\n │ └─Requires [ae029012] log:\n │   ├─possible versions are: [0.5.0-0.5.2, 1.0.0-1.0.3, 1.1.0-1.1.2] or uninstalled\n │   └─restricted to versions 1.1.2 by an explicit requirement, leaving only versions 1.1.2\n ├─restricted by compatibility requirements with Flux [587475ba] to versions: [0.6.1-0.6.3, 0.7.0-0.7.5] or uninstalled, leaving only versions: [0.6.1-0.6.3, 0.7.0-0.7.5]\n │ └─Flux [587475ba] log:\n │   ├─possible versions are: [0.4.1, 0.5.0-0.5.4, 0.6.0-0.6.10, 0.7.0-0.7.3, 0.8.0-0.8.3, 0.9.0, 0.10.0-0.10.4, 0.11.0-0.11.3] or uninstalled\n │   └─restricted to versions 0.11.3 by an explicit requirement, leaving only versions 0.11.3\n ├─restricted by compatibility requirements with DataStructures [864edb3b] to versions: [0.1.0-0.1.1, 0.7.0-0.7.5] or uninstalled, leaving only versions: 0.7.0-0.7.5\n │ └─DataStructures [864edb3b] log:\n │   ├─possible versions are: [0.9.0, 0.10.0, 0.11.0-0.11.1, 0.12.0, 0.13.0, 0.14.0-0.14.1, 0.15.0, 0.16.1, 0.17.0-0.17.20, 0.18.0-0.18.8] or uninstalled\n │   └─restricted to versions 0.18.8 by an explicit requirement, leaving only versions 0.18.8\n └─restricted by compatibility requirements with CUDA [052768ef] to versions: [0.1.0-0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.5.0-0.5.2, 0.6.0] or uninstalled — no versions left\n   └─CUDA [052768ef] log:\n     ├─possible versions are: [0.1.0, 1.0.0-1.0.2, 1.1.0, 1.2.0-1.2.1, 1.3.0-1.3.3, 2.0.0-2.0.2, 2.1.0, 2.2.0-2.2.1, 2.3.0, 2.4.0] or uninstalled\n     └─restricted to versions 2.4.0 by an explicit requirement, leaving only versions 2.4.0", nothing)
└ @ Pkg.Operations /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1427
Status `/tmp/jl_3agWGi/Project.toml`
  [79e6a3ab] Adapt v2.4.0
  [88cd18e8] ConsoleProgressMonitor v0.1.2
  [82cc6244] DataInterpolations v3.3.0
  [bcd4f6db] DelayDiffEq v5.28.1
  [2b5f629d] DiffEqBase v6.53.4
  [459566f4] DiffEqCallbacks v2.16.0
  [aae7a2af] DiffEqFlux v1.30.0
  [41bf760c] DiffEqSensitivity v6.40.0
  [163ba53b] DiffResults v1.0.3
  [b4f34e82] Distances v0.10.0
  [31c24e10] Distributions v0.24.10
  [ced4e74d] DistributionsAD v0.6.15
  [587475ba] Flux v0.11.3
  [f6369f11] ForwardDiff v0.10.14
  [a75be94c] GalacticOptim v0.4.4
  [7e08b658] GeometricFlux v0.7.5
  [e6f89c97] LoggingExtras v0.4.2
  [76087f3c] NLopt v0.6.1
  [429524aa] Optim v1.2.2
  [1dea7af3] OrdinaryDiffEq v5.49.1
  [33c8b6b6] ProgressLogging v0.1.3
  [731186ca] RecursiveArrayTools v2.10.0
  [ae029012] Requires v1.1.2
  [37e2e3b7] ReverseDiff v1.5.0
  [1bc83da4] SafeTestsets v0.0.1
  [90137ffa] StaticArrays v1.0.1
  [789caeaf] StochasticDiffEq v6.30.1
  [5d786b92] TerminalLoggers v0.1.2
  [e88e6eb3] Zygote v0.5.17
  [700de1a5] ZygoteRules v0.2.1
  [8ba89e20] Distributed
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [44cfe95a] Pkg
  [de0858da] Printf
  [9a3f8284] Random
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_3agWGi/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v2.4.0
  [ec485272] ArnoldiMethod v0.0.2
  [4fba245c] ArrayInterface v2.14.13
  [4c555306] ArrayLayouts v0.4.12
  [56f22d72] Artifacts v1.3.0
  [ab4f0b2a] BFloat16s v0.1.0
  [6e4b80f9] BenchmarkTools v0.5.0
  [b99e7846] BinaryProvider v0.5.10
  [a74b3585] Blosc v0.7.0
  [0b7ba130] Blosc_jll v1.14.3+1
  [e1450e63] BufferedStreams v1.0.0
  [6e34b625] Bzip2_jll v1.0.6+5
  [fa961155] CEnum v0.4.1
  [336ed68f] CSV v0.8.2
  [052768ef] CUDA v2.2.1
  [082447d4] ChainRules v0.7.45
  [d360d2e6] ChainRulesCore v0.9.24
  [523fee87] CodecBzip2 v0.7.2
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.9
  [5ae59095] Colors v0.12.6
  [bbf7d656] CommonSubexpressions v0.3.0
  [34da2185] Compat v3.25.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.4+0
  [8f4d0f93] Conda v1.5.0
  [88cd18e8] ConsoleProgressMonitor v0.1.2
  [187b0558] ConstructionBase v1.0.0
  [adafc99b] CpuId v0.2.2
  [9a962f9c] DataAPI v1.4.0
  [124859b0] DataDeps v0.7.6
  [82cc6244] DataInterpolations v3.3.0
  [864edb3b] DataStructures v0.18.8
  [e2d170a0] DataValueInterfaces v1.0.0
  [bcd4f6db] DelayDiffEq v5.28.1
  [2b5f629d] DiffEqBase v6.53.4
  [459566f4] DiffEqCallbacks v2.16.0
  [aae7a2af] DiffEqFlux v1.30.0
  [c894b116] DiffEqJump v6.12.2
  [77a26b50] DiffEqNoiseProcess v5.5.0
  [41bf760c] DiffEqSensitivity v6.40.0
  [163ba53b] DiffResults v1.0.3
  [b552c78f] DiffRules v1.0.2
  [b4f34e82] Distances v0.10.0
  [31c24e10] Distributions v0.24.10
  [ced4e74d] DistributionsAD v0.6.15
  [ffbed154] DocStringExtensions v0.8.3
  [d4d017d3] ExponentialUtilities v1.8.0
  [e2ba6199] ExprTools v0.1.3
  [7a1cc6ca] FFTW v1.3.0
  [f5851436] FFTW_jll v3.3.9+7
  [9aa1b823] FastClosures v0.3.2
  [1a297f60] FillArrays v0.10.2
  [6a86dc24] FiniteDiff v2.7.2
  [53c48c17] FixedPointNumbers v0.8.4
  [587475ba] Flux v0.11.3
  [f6369f11] ForwardDiff v0.10.14
  [069b7b12] FunctionWrappers v1.1.1
  [d9f16b24] Functors v0.1.0
  [0c68f7d7] GPUArrays v6.2.0
  [61eb1bfa] GPUCompiler v0.8.3
  [a75be94c] GalacticOptim v0.4.4
  [01680d73] GenericSVD v0.3.0
  [7e08b658] GeometricFlux v0.7.5
  [af5da776] GlobalSensitivity v0.0.1
  [a1251efa] GraphLaplacians v0.1.1
  [21828b05] GraphMLDatasets v0.1.2
  [3ebe565e] GraphSignals v0.1.12
  [f67ccb44] HDF5 v0.14.3
  [0234f1f7] HDF5_jll v1.12.0+1
  [cd3eb016] HTTP v0.9.2
  [7869d1d1] IRTools v0.4.2
  [d25df0c9] Inflate v0.1.2
  [83e8ac13] IniFile v0.5.0
  [1d5cc7b8] IntelOpenMP_jll v2018.0.3+0
  [42fd0dbc] IterativeSolvers v0.9.0
  [82899510] IteratorInterfaceExtensions v1.0.0
  [033835bb] JLD2 v0.2.4
  [692b3bcd] JLLWrappers v1.2.0
  [682c06a0] JSON v0.21.1
  [7d188eb4] JSONSchema v0.3.3
  [e5e0dc1b] Juno v0.8.4
  [929cbde3] LLVM v3.5.2
  [2ee39098] LabelledArrays v1.4.0
  [a5e1c1ea] LatinHypercubeSampling v1.7.3
  [73f95e8e] LatticeRules v0.0.1
  [1d6d02ad] LeftChildRightSiblingTrees v0.1.2
  [deac9b47] LibCURL_jll v7.70.0+2
  [29816b5a] LibSSH2_jll v1.9.0+3
  [093fc24a] LightGraphs v1.3.4
  [d3d80556] LineSearches v7.1.1
  [e6f89c97] LoggingExtras v0.4.2
  [bdcacae8] LoopVectorization v0.8.26
  [5ced341a] Lz4_jll v1.9.2+2
  [23992714] MAT v0.9.2
  [856f044c] MKL_jll v2020.2.254+0
  [1914dd2f] MacroTools v0.5.6
  [b8f27783] MathOptInterface v0.9.19
  [fdba3010] MathProgBase v0.7.8
  [739be429] MbedTLS v1.0.3
  [c8ffd9c3] MbedTLS_jll v2.16.8+1
  [e89f7d12] Media v0.5.0
  [626554b9] MetaGraphs v0.6.6
  [e1d29d7a] Missings v0.4.4
  [46d2c3a1] MuladdMacro v0.2.2
  [d8a4904e] MutableArithmetics v0.2.13
  [d41bc354] NLSolversBase v7.7.1
  [76087f3c] NLopt v0.6.1
  [079eb43e] NLopt_jll v2.7.0+0
  [2774e3e8] NLsolve v4.5.1
  [872c559c] NNlib v0.7.11
  [77ba4419] NaNMath v0.3.5
  [8913a72c] NonlinearSolve v0.3.4
  [6fe1bfb0] OffsetArrays v1.5.0
  [458c3c95] OpenSSL_jll v1.1.1+6
  [efe28fd5] OpenSpecFun_jll v0.5.3+4
  [429524aa] Optim v1.2.2
  [bac558e1] OrderedCollections v1.3.2
  [1dea7af3] OrdinaryDiffEq v5.49.1
  [90014a1f] PDMats v0.10.1
  [d96e819e] Parameters v0.12.1
  [69de0a69] Parsers v1.0.15
  [e409e4f3] PoissonRandom v0.4.0
  [2dfb63ee] PooledArrays v0.5.3
  [85a6dd25] PositiveFactorizations v0.2.4
  [33c8b6b6] ProgressLogging v0.1.3
  [92933f4c] ProgressMeter v1.4.1
  [438e738f] PyCall v1.92.2
  [1fd47b50] QuadGK v2.4.1
  [8a4e6c94] QuasiMonteCarlo v0.2.2
  [74087812] Random123 v1.2.0
  [e6cf234a] RandomNumbers v1.4.0
  [3cdcf5f2] RecipesBase v1.1.1
  [731186ca] RecursiveArrayTools v2.10.0
  [f2c3362d] RecursiveFactorization v0.1.6
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.1.2
  [ae5879a3] ResettableStacks v1.1.0
  [37e2e3b7] ReverseDiff v1.5.0
  [79098fc4] Rmath v0.6.1
  [f50d1b31] Rmath_jll v0.2.2+1
  [21efa798] SIMDPirates v0.8.26
  [476501e8] SLEEFPirates v0.5.5
  [1bc83da4] SafeTestsets v0.0.1
  [b1168b60] ScatterNNlib v0.1.6
  [6c6a2e73] Scratch v1.0.3
  [91c51154] SentinelArrays v1.2.16
  [efcf1570] Setfield v0.7.0
  [699a6c99] SimpleTraits v0.9.3
  [47aef6b3] SimpleWeightedGraphs v1.1.1
  [ed01d8cd] Sobol v1.4.0
  [a2af1166] SortingAlgorithms v0.3.1
  [47a9eef4] SparseDiffTools v1.12.0
  [276daf66] SpecialFunctions v1.2.1
  [860ef19b] StableRNGs v1.0.0
  [90137ffa] StaticArrays v1.0.1
  [2913bbd2] StatsBase v0.33.2
  [4c63d2b9] StatsFuns v0.9.6
  [789caeaf] StochasticDiffEq v6.30.1
  [3783bdb8] TableTraits v1.0.0
  [bd369af6] Tables v1.2.2
  [5d786b92] TerminalLoggers v0.1.2
  [a759f4b9] TimerOutputs v0.5.7
  [9f7883ad] Tracker v0.2.14
  [3bb67fe8] TranscodingStreams v0.9.5
  [a2a6695c] TreeViews v0.3.0
  [5c2747f8] URIs v1.1.0
  [3a884ed6] UnPack v1.0.2
  [3d5dd08c] VectorizationBase v0.12.33
  [81def892] VersionParsing v1.2.0
  [19fa3120] VertexSafeGraphs v0.1.2
  [a5390f91] ZipFile v0.9.3
  [83775a58] Zlib_jll v1.2.11+18
  [3161d3a3] Zstd_jll v1.4.5+2
  [e88e6eb3] Zygote v0.5.17
  [700de1a5] ZygoteRules v0.2.1
  [8e850ede] nghttp2_jll v1.40.0+2
  [3f19e933] p7zip_jll v16.2.0+3
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8bb1440f] DelimitedFiles
  [8ba89e20] Distributed
  [9fa8497b] Future
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [9abbd945] Profile
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [1a1011a3] SharedArrays
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [4607b0f0] SuiteSparse
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
Starting tests
416.7086995514811301.86465432569827213.21697213391016149.3929796071194115.2247517913104296.3438136311290279.3936123278511765.5625775751988754.73408970504229446.2626159904189439.55562467607837434.29338639796445430.54262397471921328.93102825992030330.78230280655101637.5616921386252547.3899178101139352.31771747076819448.8559023431533241.1409918445188933.4998573173513127.7136171838481823.8213532431337521.2574969683678819.43364473086034417.9080234863235616.40169427585258414.75827680393393112.9155083938401710.8813206837866058.7182734287630626.5317427624672214.45717155975515762.6440088213855261.23592071815229730.34657206429314730.032376452495214510.26367482940779230.905567097641851.72975726545156162.4738072183786732.93352765725007243.03618679634409672.84498125290393672.49864919108605132.1348949032518721.84171784634708581.64735382952383951.53548374223780541.46787860302471571.40348733006078131.31057624087927451.1723646987133380.98792068128521930.77000396330597080.54099857867771020.327773302321376360.156017818275280080.0445653704883056740.00091322139101779030.0190239786206280320.080410475436539270.159197271761418070.229589835455646730.27324885152407110.28364198270243850.26573545746272670.231707259387955280.195075966066423880.16585180213779310.148215404734272720.140797690163480650.13867239213660060.135911948945067140.127791420961522520.112133203758811030.089657382441687130.063454110359997350.037871264121909730.017167018654110830.0042911560636956050.000132331621225468740.0034254726393664930.0112719546828020260.020185633126746330.027176635651557520.0305475552150724330.0301433293399415230.027042574606087690.0228944748810245740.0192068480124802340.0168492892211720180.0158974341901012840.0157962049318957630.0157121461393411320.0149121397453862370.013034469259305310.0101844853779321530.0068527418229991870.00371107376408517140.41651472333630622.0574979950004412.0370170061059436.5135123950745713.44931932189921181.7559468701164440.84093619554414190.37442507954085170.168532382317527320.113993033837653730.146123430878245250.226085903016524930.330334854476624140.44453607046360510.55999043965512660.67148965890465460.77600896170330960.87189794211576280.95837284804221071.03519453504103541.102462468119151.1604809403635251.2096740246675781.2505296827649941.28356389938792611.3092977792750041.3282432671420641.3408945567685451.3477231917904121.34917568299961181.34567275651153921.33760965969436761.32535713923978181.30926283119076661.28965288718113881.26683371815545651.24109377565182391.21270532363075211.18192615543916561.14900120277133591.11416408494757111.0776385384441541.03963972907575551.00037545475925030.96004716055324730.91885085925083050.87697796780162950.83461578453768470.79194791465576250.7491546092974780.70641292726443780.66389667527127090.62177597710401310.58021695098207630.53938109283327550.49942448098243830.46049683701543230.422740446135237870.386288958824478560.351266106102598450.31778436152719740.28594358960323430.25582972628403930.227513541949103480.20104954040439650.17647504822376870.153809547487706370.133054301003206670.11419231258130050.097188655030507010.081991185732421780.068531653945972460.056727195199598220.0464821741737222950.037690330900355420.0302371660240337120.024002485214972780.018863008126808380.0146949697216094770.011376587915764770.008790320506736630.0068248311695943480.0053766232540579290.0043512582546466540.0036641684658992570.003241059631909680.00301792543950275570.00294072194226196620.0029647594934878470.00305388071927276130.00317949724857487840.0033195565264025470.0034575035527390830.00358129205678411840.0036824867525255790.0037554842797734490.00379686650199046730.0038048872648166380.00377908325254354550.003719992014970818416.70869955148316301.87186997430365213.22801705361297149.40648614478837115.2240412249364696.3454391345538679.3977976779125865.5678714713974554.73969630607274446.2681431647925939.5608383606454234.2980229596167530.54633410011867428.9332381463373130.78208250143966437.5587566610162847.3870414634704752.3184906512701248.8589544323429841.14383255919601433.5012140860317727.71329342894746323.81952936942857221.2543767187669819.4294856183711817.90303100693667616.39608500849614714.75230897079328312.90945584534311610.8754625577971288.712882525457966.5270686024001344.4534149599774862.6412962392444671.2342795663056390.3459158046638280.032506748634095760.264310414146053160.90639787468961761.7305156642179892.4743407762289472.9338239544033673.0363286740031682.84506488225275162.49872238818174442.13494430274363721.8416920826943291.6471951659831451.53515198186780831.46736343542057451.40280885504569251.30977827422533061.1715059590677530.98706641658549010.76921762520158130.54033333964074280.32726603243509670.155685347538229820.044402053227207340.00089265358725995560.0191039242369983630.080534769824929170.159307303717266780.229641600525029880.273224851436197740.283552996888048250.26561206968046450.23158584804701470.194985613398158870.165806707487119760.14821387238102470.140826097601411540.13871147379454670.135943057155115480.12780127946737040.112116466759106760.089616889144780110.063399129505176940.037814459743258160.017120978004210560.00426522788223897450.00013060182132885350.00344585479851722280.011306698777051870.0202248123961467770.02721155913652230.0305730930624841660.030158440940405340.027049241017987240.022895782224433470.0192051454721658860.016845112348903880.0158894783750984330.015782221974403340.015690315650227170.0148822708785586060.0129985681020147110.0101464819170533940.0068175358724189420.003683289147449469Test Summary: | Pass  Total
Layers Tests  |    6      6
Test Summary: | Pass  Total
Fast Layers   |    9      9
494.2089032199089416.70869955148316301.87186997430365213.22801705361297149.40648614478837115.2240412249364696.3454391345538679.3977976779125865.5678714713974554.73969630607274446.2681431647925939.5608383606454234.2980229596167530.54633410011867428.9332381463373130.78208250143966437.5587566610162847.3870414634704752.3184906512701248.8589544323429841.14383255919601433.5012140860317727.71329342894746323.81952936942857221.2543767187669819.4294856183711817.90303100693667616.39608500849614714.75230897079328312.90945584534311610.8754625577971288.712882525457966.5270686024001344.4534149599774862.6412962392444671.2342795663056390.3459158046638280.032506748634095760.264310414146053160.90639787468961761.7305156642179892.4743407762289472.9338239544033673.0363286740031682.84506488225275162.49872238818174442.13494430274363721.8416920826943291.6471951659831451.53515198186780831.46736343542057451.40280885504569251.30977827422533061.1715059590677530.98706641658549010.76921762520158130.54033333964074280.32726603243509670.155685347538229820.044402053227207340.00089265358725995560.0191039242369983630.080534769824929170.159307303717266780.229641600525029880.273224851436197740.283552996888048250.26561206968046450.23158584804701470.194985613398158870.165806707487119760.14821387238102470.140826097601411540.13871147379454670.135943057155115480.12780127946737040.112116466759106760.089616889144780110.063399129505176940.037814459743258160.017120978004210560.00426522788223897450.00013060182132885350.00344585479851722280.011306698777051870.0202248123961467770.02721155913652230.0305730930624841660.030158440940405340.027049241017987240.022895782224433470.0192051454721658860.016845112348903880.0158894783750984330.015782221974403340.015690315650227170.0148822708785586060.0129985681020147110.0101464819170533940.0068175358724189420.0001306018213288535494.2089032199089473.8777272120393667.1074203352925518.2054619170083578.8154382368062646.0053562468045710.16231672434129890.00118010501859213252.6199969581810063e-72.6199995221555827e-7494.2089032199089473.732958217371967.6525410631349228.2939447105199618.918170043729313.9279108564900232.35248343766358660.046835997229179536.353324108127963e-56.901903337638466e-50.0114590337976664260.00081457673100001553.635453873943374e-69.694315777963093e-71.1767027710477412e-69.694315777963093e-73.4794595759808757e-71.5536009694147404e-71.5957294409569188e-71.5536009694147404e-71.5957294409569188e-71.5957294409569188e-71.5957298549742173e-774.2333305814717940.41651472333630622.0574979950004412.0370170061059436.5135123950745713.44931932189921181.7559468701164440.84093619554414190.37442507954085170.168532382317527320.113993033837653730.146123430878245250.226085903016524930.330334854476624140.44453607046360510.55999043965512660.67148965890465460.77600896170330960.87189794211576280.95837284804221071.03519453504103541.102462468119151.1604809403635251.2096740246675781.2505296827649941.28356389938792611.3092977792750041.3282432671420641.3408945567685451.3477231917904121.34917568299961181.34567275651153921.33760965969436761.32535713923978181.30926283119076661.28965288718113881.26683371815545651.24109377565182391.21270532363075211.18192615543916561.14900120277133591.11416408494757111.0776385384441541.03963972907575551.00037545475925030.96004716055324730.91885085925083050.87697796780162950.83461578453768470.79194791465576250.7491546092974780.70641292726443780.66389667527127090.62177597710401310.58021695098207630.53938109283327550.49942448098243830.46049683701543230.422740446135237870.386288958824478560.351266106102598450.31778436152719740.28594358960323430.25582972628403930.227513541949103480.20104954040439650.17647504822376870.153809547487706370.133054301003206670.11419231258130050.097188655030507010.081991185732421780.068531653945972460.056727195199598220.0464821741737222950.037690330900355420.0302371660240337120.024002485214972780.018863008126808380.0146949697216094770.011376587915764770.008790320506736630.0068248311695943480.0053766232540579290.0043512582546466540.0036641684658992570.003241059631909680.00301792543950275570.00294072194226196620.0029647594934878470.00305388071927276130.00317949724857487840.0033195565264025470.0034575035527390830.00358129205678411840.0036824867525255790.0037554842797734490.00379686650199046730.0038048872648166380.00377908325254354550.002940721942261966274.233330581471791.56137418678787941.51944967050779931.51474756200861660.68245297239012170.19137638568159760.000192859980237446451.788836458252143e-61.0547470544227319e-105.254137262629107e-171.8119148916795115e-29494.2089032199089416.70869955148316301.87186997430365213.22801705361297149.40648614478837115.2240412249364696.3454391345538679.3977976779125865.5678714713974554.73969630607274446.2681431647925939.5608383606454234.2980229596167530.54633410011867428.9332381463373130.78208250143966437.5587566610162847.3870414634704752.3184906512701248.8589544323429841.14383255919601433.5012140860317727.71329342894746323.81952936942857221.2543767187669819.4294856183711817.90303100693667616.39608500849614714.75230897079328312.90945584534311610.8754625577971288.712882525457966.5270686024001344.4534149599774862.6412962392444671.2342795663056390.3459158046638280.032506748634095760.264310414146053160.90639787468961761.7305156642179892.4743407762289472.9338239544033673.0363286740031682.84506488225275162.49872238818174442.13494430274363721.8416920826943291.6471951659831451.53515198186780831.46736343542057451.40280885504569251.30977827422533061.1715059590677530.98706641658549010.76921762520158130.54033333964074280.32726603243509670.155685347538229820.044402053227207340.00089265358725995560.0191039242369983630.080534769824929170.159307303717266780.229641600525029880.273224851436197740.283552996888048250.26561206968046450.23158584804701470.194985613398158870.165806707487119760.14821387238102470.140826097601411540.13871147379454670.135943057155115480.12780127946737040.112116466759106760.089616889144780110.063399129505176940.037814459743258160.017120978004210560.00426522788223897450.00013060182132885350.00344585479851722280.011306698777051870.0202248123961467770.02721155913652230.0305730930624841660.030158440940405340.027049241017987240.022895782224433470.0192051454721658860.016845112348903880.0158894783750984330.015782221974403340.015690315650227170.0148822708785586060.0129985681020147110.0101464819170533940.0068175358724189420.0001306018213288535494.2089032199089473.853379851477267.1910394712543619.3668972750808739.7656628835067256.7213112337089560.210802099219018020.0020064861760209748.733413153743843e-78.643809856106826e-98.541033704918051e-98.361164356847039e-98.317372464516128e-98.267710578974257e-98.216170538005704e-98.173508817546354e-98.063992267734478e-98.01162363901422e-97.99607035692014e-97.995962747971687e-97.995960116772822e-97.995967489500663e-97.99597547131374e-97.995662408675744e-97.995669375120758e-97.995673048410948e-97.995676395300453e-97.99568327296117e-97.99569002316523e-97.99569746035344e-97.995701886601305e-97.9957040481837e-97.995709575241539e-97.995712114952812e-97.995716473122468e-97.995723291744883e-97.99572691195183e-97.995734716823819e-97.995739182586709e-97.995741237207654e-97.995749174284868e-97.995748905929765e-97.995755232736463e-97.995760664785721e-97.995762348628909e-97.995767302157528e-97.995771370158946e-97.995777275180081e-97.995779661491644e-97.99578162003234e-97.995785794246537e-97.995792779685345e-97.995793743178586e-97.995798981400384e-97.995803539989782e-97.995807902980197e-97.995811511198476e-97.995810863138465e-97.99581423357792e-97.995820014805759e-97.995826173766786e-97.99583341659563e-97.995837475940589e-97.995837312443771e-97.995837312443771e-9494.2089032199089473.708957785740967.7761833565412430.5372695141565522.04440028004954416.404636288026654.9603066990716880.194591219854741950.004058851617001157.147148065319896e-55.9510123639957265e-50.000174518373440634830.0092706812141542420.0030222390251977141.0267359218644397e-51.4522343521301269e-67.686833463033755e-73.600496406843956e-74.4080691381235183e-73.383496109152047e-72.997152385067193e-78.121733108639196e-60.000221220080970844770.000224682794264422385.115195482952544e-52.5246311087766757e-71.658383660753623e-71.6373827534887686e-71.617313496531134e-71.617313496531134e-71.6418100153072227e-71.627983834787014e-71.6278472928886177e-71.622205096529859e-71.6184830275795784e-71.6202521100770352e-71.6192994667779848e-71.617701806125376e-71.6178952685705337e-71.6170325502039833e-71.6170337698312493e-71.6170127737038035e-71.616995626495644e-71.6170023507994416e-71.61689289041066e-71.6167978864730173e-71.616631790217669e-71.6165370513116566e-71.6163597445265485e-71.616385441563922e-71.616768432566055e-71.6165198001880525e-71.6179955219939407e-71.6168228608773648e-71.6244289819974508e-71.6370822034695743e-71.6431092303357142e-71.6674359270248463e-71.6234428150324763e-71.6234428150324763e-71.6211570028014825e-71.6209513912125766e-71.620792332612904e-71.6186159331785565e-71.6174685856950145e-71.6175081825339462e-71.6175471419503582e-71.6175471419503582e-71.616263516958333e-71.6160566488838499e-71.6158819738258231e-71.6158828263356447e-71.615884102814833e-71.615884102814833e-71.6156418250349603e-71.615510819833142e-71.6154674672019237e-71.615468232636376e-71.6154261988320252e-71.6154274208162867e-71.6154274208162867e-71.6154268565361845e-71.6154275268917293e-71.615351523485621e-71.6153527413086692e-71.6153537626333673e-71.6153537626333673e-71.615354138316092e-71.615355339353219e-71.6153555031848379e-71.6153555031848379e-71.615355251719032e-71.6153563315234064e-71.6153574946852075e-71.615356192900127e-71.615356847619923e-71.615356847619923e-71.6153549950763138e-71.6153556403305155e-71.6153567354890933e-71.6153569655059297e-71.6153569655059297e-71.6153520457967951e-71.6153532226424755e-71.61535394509749e-71.6153553624345332e-71.6153553624345332e-71.6153512613802895e-71.6153527152583922e-71.6153538611332498e-71.615354412162688e-71.6153556977944278e-71.6153556977944278e-71.6153556371528738e-71.6153563218904874e-71.6153522464103947e-71.615353398646347e-71.615353398646347e-71.6153538987552076e-71.6153479618046807e-71.6153495763174002e-71.6153495763174002e-71.6153510735076292e-71.615352225233006e-71.615352359535805e-71.6153539744532424e-71.6153539744532424e-71.6153489865193656e-71.6153505991276366e-71.615351682094142e-71.6153522265748675e-71.615352456787326e-71.6153532531634003e-71.6153532531634003e-71.615352860094826e-71.615354475374803e-71.6153556733403397e-71.6153567993325209e-71.6153567993325209e-71.61535752249614e-71.6153587516411636e-71.615359914180785e-71.6153458921725698e-71.6153458921725698e-71.615346699016413e-71.6153475722187842e-71.6153485715241128e-71.6153485715241128e-71.6153472327208884e-71.6153479290056528e-71.6153482845328204e-71.6153487017145276e-71.6153500145071232e-71.6153500145071232e-71.6153510816324778e-71.6153525555078433e-71.615349889100664e-71.615351503615881e-71.615353005131499e-71.615353005131499e-71.6153454159938873e-71.6153468840389835e-71.6153469545701517e-71.6153469545701517e-71.6153449767331242e-71.6153462099888634e-71.6153477329161382e-71.6153492580499782e-71.6153492580499782e-71.6153459487718344e-71.6153474326359352e-71.6153483375302097e-71.615343534480381e-71.6153449749967305e-71.61534305441681e-71.6153438048579065e-71.6153447756640818e-71.615346390412961e-71.615346390412961e-71.615347714839545e-71.6153451543667691e-71.6153461965120228e-71.6153461965120228e-71.6153474728454482e-71.6153490874043246e-71.6153507026023638e-71.6153507026023638e-71.615338132108385e-71.615339746540621e-71.6153409932081768e-71.6153397033712106e-71.6153400611269582e-71.6153400611269582e-71.6153415755474105e-71.6153414413320063e-71.6153424838914903e-71.6153424838914903e-71.6153396426250628e-71.615341257568645e-71.6153428728680176e-71.6153425580307392e-71.615344173245751e-71.615344173245751e-71.6153356694787596e-71.6153372840979217e-71.6153386454999824e-71.6153386454999824e-71.615335277644336e-71.615336877575755e-71.6153352107760183e-71.615335846958087e-71.615336744795237e-71.6153379648876818e-71.6153379648876818e-71.6153332400112389e-71.6153339773793926e-71.61533530619655e-71.61533530619655e-71.6153320901681433e-71.6153334733875456e-71.6153344295079261e-71.6153344295079261e-71.6153311595651954e-71.6153325107451323e-71.6153335233750968e-71.6153335233750968e-71.615331626592291e-71.6153324684428524e-71.6153334260166798e-71.6153334260166798e-71.6153280909193748e-71.6153297057630316e-71.615331304707733e-71.615331304707733e-71.6153307646404597e-71.6153323668331498e-71.6153331101647523e-71.6153331101647523e-71.6153313336911435e-71.6153325496327542e-71.6153314597383173e-71.6153291932523552e-71.6153308077795584e-71.6153308077795584e-71.615331551017958e-71.6153331658235916e-71.6153338706473428e-71.615335023543526e-71.615335023543526e-71.6153294532823335e-71.6153310680218928e-71.6153326826487103e-71.6153326826487103e-71.61532689172708e-71.6153271946562e-71.6153278875733768e-71.6153278875733768e-71.6153234095536003e-71.6153243400842052e-71.6153259116877276e-71.615327185768902e-71.615327185768902e-71.6153202314385944e-71.6153218449698115e-71.6153217737063933e-71.6153222958915115e-71.61532248415465e-71.61532248415465e-71.6153223721750181e-71.6153239865864102e-71.61532547801728e-71.61532547801728e-71.6153254757647132e-71.6153264458011577e-71.61532806018049e-71.61532806018049e-71.615322032356971e-71.615322203904612e-71.6153234454871805e-71.6153234454871805e-71.615323480313273e-71.615324936262222e-71.615326165116769e-71.6153274664279575e-71.6153274664279575e-71.615324893336885e-71.615325421309044e-71.615327035376395e-71.6153278800175626e-71.6153278800175626e-71.6153228324476842e-71.6153231722259563e-71.6153245394776647e-71.6153245394776647e-71.6153233245933356e-71.615324495821995e-71.6153250506506052e-71.6153262126981842e-71.6153278274908123e-71.6153278274908123e-71.6153244530648735e-71.615325150167149e-71.6153264374236847e-71.6153264374236847e-71.6153113399863392e-71.6153129552081857e-71.615313380520267e-71.6153146225515348e-71.6153146225515348e-71.6153102075116469e-71.6153112462842205e-71.6153115047248822e-71.6153122056512717e-71.6153122056512717e-71.6153069114632262e-71.6153073774594988e-71.6153088294597406e-71.615308526974776e-71.615308526974776e-71.6153069570042933e-71.615307132759902e-71.6153084649742758e-71.615305502325397e-71.61530643207106e-71.61530643207106e-71.6153044039150194e-71.6153059322903374e-71.6153075465403728e-71.61530776609745e-71.61530776609745e-71.6153090110769378e-71.615309820583902e-71.6153114358044066e-71.6153114358044066e-71.615295847080446e-71.615296663326916e-71.6152961879774673e-71.6152973165647348e-71.6152973165647348e-71.6152778336701392e-71.6152794476790593e-71.615280093072967e-71.615280093072967e-71.6152773555186844e-71.6152787510181062e-71.6152803661200112e-71.6152803661200112e-71.6152811750049686e-71.6152826450375926e-71.6152835448171285e-71.6152835448171285e-71.615277520960448e-71.6152784788557558e-71.6152797771318558e-71.6152761727327152e-71.61527714934241e-71.61527714934241e-71.6152783050024557e-71.6152775288084215e-71.6152784290795717e-71.6152793705901342e-71.6152793705901342e-71.6152772168724908e-71.6152783531359725e-71.61527955989798e-71.6152806102747296e-71.6152806102747296e-71.615280125253513e-71.6152817399217067e-71.6152833546749282e-71.61528471083763e-71.61528471083763e-71.6152787206694467e-71.6152800704212789e-71.6152816849455524e-71.6152825616327951e-71.6152825616327951e-71.6152807007807794e-71.6152712579668514e-71.6152727407576935e-71.61527435594079e-71.6152759698696004e-71.6152759698696004e-71.615267517588116e-71.61526817177605e-71.6152690910239493e-71.615270706174908e-71.615270706174908e-71.6152642727253105e-71.6152658873391212e-71.6152664279072848e-71.6152664279072848e-71.6152607824979532e-71.6152610584188955e-71.6152626729829915e-71.6152626729829915e-71.6151637258617565e-71.615165340967188e-71.615166955603241e-71.6151682388379047e-71.6151682388379047e-71.6151696757253785e-71.615170847816435e-71.6151721184000302e-71.6151721184000302e-71.6151579802459787e-71.6151595952356945e-71.6151576541103682e-71.6151552861372774e-71.6151569009194176e-71.6151574653967834e-71.6151574653967834e-71.615152220172684e-71.6151536338577263e-71.6151548284638261e-71.6151548284638261e-71.6151411354807064e-71.6151425547018836e-71.6151441697782722e-71.6151441697782722e-71.6151358895174646e-71.6151369649459382e-71.615138579637156e-71.615138579637156e-71.6151307368377253e-71.6151321677516237e-71.6151333856303694e-71.6151350003263466e-71.6151350003263466e-71.615127346858423e-71.6151289618846006e-71.6151303391780336e-71.615131954096506e-71.615131954096506e-71.6151237503124372e-71.6151247394273496e-71.6151258625516397e-71.6151262428613788e-71.6151278577127846e-71.6151278577127846e-71.6151259197765016e-71.6151275348548517e-71.615129074572377e-71.6151288490207892e-71.6151304487051161e-71.6151304487051161e-71.615111181598072e-71.615112669645421e-71.6151137987983665e-71.6151154134272022e-71.6151154134272022e-71.6150901570021577e-71.6150916114398038e-71.6150923741982633e-71.6150939888554257e-71.6150939888554257e-71.6150956038112892e-71.615097218567876e-71.6150974514028104e-71.6150974514028104e-71.6150914560188067e-71.615092301049095e-71.6150935629640813e-71.615055018639962e-71.615056214900513e-71.615056214900513e-71.6150323392454243e-71.6150338179632674e-71.6150347651919143e-71.6149985428397196e-71.6149985428397196e-71.6149875240578875e-71.6149891383277686e-71.614990752886815e-71.6149923677092547e-71.6149923677092547e-71.6149807719754738e-71.614982288897745e-71.614983455775526e-71.6149802732113714e-71.6149802732113714e-71.6149805235052758e-71.6149811717892526e-71.6149826009667793e-71.614983699507631e-71.614983699507631e-71.614978550570448e-71.6149792760285043e-71.6149805264721778e-71.6149812731108978e-71.6149812731108978e-71.6149732406886324e-71.6149748540352736e-71.6149761669893772e-71.6149774274799568e-71.6149774274799568e-71.614979042358509e-71.6149804992267145e-71.614982113289882e-71.614982113289882e-71.6149832549864546e-71.6149848694357392e-71.6149864841425092e-71.6149864841425092e-71.6149797775785875e-71.6149813918888518e-71.6149825602544522e-71.6149830810741054e-71.6149830810741054e-71.6149827464303173e-71.614984099197127e-71.614984868958285e-71.614984868958285e-71.614973003168883e-71.6149741361519047e-71.614975667501289e-71.6149768674440192e-71.6149768674440192e-71.6149625529965808e-71.6149638298116563e-71.6149654444512158e-71.6149654444512158e-71.6149623065015014e-71.6149635718384578e-71.614964915320796e-71.614964915320796e-71.6149644669021346e-71.614965191575524e-71.614966670411331e-71.6149681607766503e-71.6149681607766503e-71.614963123227951e-71.6149645923209153e-71.6149605710678658e-71.6149621859219176e-71.6149637999767993e-71.6149637999767993e-71.6149603804259558e-71.614961593330829e-71.6149632076607298e-71.6149648216108728e-71.6149648216108728e-71.6149594304037984e-71.614960933987813e-71.6149618412601652e-71.6149632819143768e-71.6149632819143768e-71.6149573544016993e-71.6149584077442653e-71.6149589609920762e-71.6149602880475658e-71.6149602880475658e-71.614957521761007e-71.6149591362217954e-71.6149595737778933e-71.6149604618191124e-71.6149604618191124e-71.614954627320131e-71.614955582474607e-71.614957163032197e-71.614957163032197e-71.6149533493381015e-71.6149547327265549e-71.6149561804177074e-71.6149571236140666e-71.614957828944246e-71.614957828944246e-71.614956869710803e-71.614958461216823e-71.6149580818003362e-71.6149592487219834e-71.6149607010080692e-71.6149607010080692e-71.614955066955544e-71.6149557943547127e-71.6149574089028093e-71.6149590237292594e-71.6149590237292594e-71.614956214424614e-71.6149564508716844e-71.6149575993231485e-71.614958125233181e-71.614958125233181e-71.6149493145874327e-71.614950929442374e-71.6149525437396706e-71.6149505890202093e-71.6149519539119197e-71.6149519539119197e-71.614952115713847e-71.6149534900441429e-71.6149541923804997e-71.6149551466757352e-71.6149551466757352e-71.6148761129100332e-71.6148769602124423e-71.6148784641010955e-71.6148784641010955e-71.6148743701039862e-71.6148759844683343e-71.614876257619522e-71.6148778719985845e-71.6148778719985845e-71.614868755760789e-71.6148696687084146e-71.614869820262697e-71.614869820262697e-71.6148670721227468e-71.6148675906249943e-71.6148682004314244e-71.6148682004314244e-71.6148648982024854e-71.6148665119396848e-71.6148676605159941e-71.6148692749884136e-71.6148692749884136e-71.6148644985650495e-71.6148658242692826e-71.614860555483718e-71.6148596243177178e-71.6148608858077004e-71.6148619814213573e-71.6148619814213573e-71.614863596247274e-71.6148652109525505e-71.614866825445751e-71.614866825445751e-71.6148605988930243e-71.6148618860531697e-71.614863100358294e-71.614863100358294e-71.6148606884807057e-71.6148620830888727e-71.6148636110069404e-71.6148602323689081e-71.6148616543248288e-71.6148616543248288e-71.614858976618824e-71.61486031006627e-71.6148617067648845e-71.6148630691074647e-71.6148630691074647e-71.6148576120522624e-71.6148592267833697e-71.6148608404062535e-71.6148608404062535e-71.614854400767317e-71.614855277116647e-71.6148566183316552e-71.6148578823514664e-71.6148578823514664e-71.6148569071417692e-71.614858521795854e-71.6148600321517057e-71.6148600321517057e-71.6148548293524253e-71.6148558145170632e-71.6148567042658095e-71.6148567042658095e-71.614853966132213e-71.614855327080323e-71.6148559744854996e-71.614855826982495e-71.614855826982495e-71.6148572353283258e-71.6148583711775644e-71.6148595871103543e-71.614860447420845e-71.614860447420845e-71.6148557727149377e-71.6148571568872275e-71.6148587715408149e-71.6148587715408149e-71.614852962721197e-71.6148543591018003e-71.614855560313054e-71.6148567855408654e-71.6148567855408654e-71.6148505669801398e-71.6148518663043856e-71.61485276890542e-71.61485276890542e-71.6148527849107685e-71.6148541008673797e-71.6148557151200661e-71.6148537913813635e-71.6148537913813635e-71.6148475379313253e-71.614848786790292e-71.6148502782923261e-71.6148502782923261e-71.6148386292354087e-71.614840243794643e-71.6148418584141082e-7┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:342
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:348
┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:342
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:328
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:348
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:328
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:328
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:348
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/znIav/src/integrator_interface.jl:328
Test Summary:       | Pass  Total
GalacticOptim Tests |   16     16
Test Summary: | Pass  Total
Layers SDE    |    3      3
Test Summary: | Pass  Total
Layers DDE    |    3      3
Test Summary:          | Pass  Total
Collocation Regression |    2      2
Starting to train
[ Info: Epoch 1
loss() = 8616.039413285986
loss() = 6704.7166955090215
loss() = 5214.271933448323
loss() = 4057.883208653414
loss() = 3163.593195625366
loss() = 2473.1559429254676
loss() = 1940.317451156917
loss() = 1528.8777106981045
loss() = 1210.8014239202807
loss() = 964.5271905288995
[ Info: Epoch 2
loss() = 773.5393071616918
loss() = 625.208353525128
loss() = 509.87550112093425
loss() = 420.1427782575062
loss() = 350.32979868657674
loss() = 296.06116979773196
loss() = 253.95453787716303
loss() = 221.385112266733
loss() = 196.30798293879033
loss() = 177.12379526009306
[ Info: Epoch 3
loss() = 162.57697048408298
loss() = 151.67833279609926
loss() = 143.64606719153153
loss() = 137.86047838819843
loss() = 133.82917250106823
loss() = 131.1601383194198
loss() = 129.5408402988063
loss() = 128.72190581175607
loss() = 128.5043389603441
loss() = 128.729453621618
[ Info: Epoch 4
loss() = 129.27091282521326
loss() = 130.02840707021787
loss() = 130.92261373164948
loss() = 131.89116242561713
loss() = 132.88538843718888
loss() = 133.8677347835274
loss() = 134.80962476961793
loss() = 135.68975120148758
loss() = 136.49268253423682
loss() = 137.20772599059922
[ Info: Epoch 5
loss() = 137.82799866350734
loss() = 138.34966719902644
loss() = 138.771324572625
loss() = 139.09347870111878
loss() = 139.3181325553004
loss() = 139.4484393396574
loss() = 139.48841941379143
loss() = 139.4427281134627
loss() = 139.31646562163957
loss() = 139.1150216431165
[ Info: Epoch 6
loss() = 138.8439489336723
loss() = 138.50886078389678
loss() = 138.11534841423514
loss() = 137.6689149340216
loss() = 137.17492308996384
loss() = 136.63855449689845
loss() = 136.064778430937
loss() = 135.45832858354186
loss() = 134.82368643820686
loss() = 134.16507015119848
[ Info: Epoch 7
loss() = 133.48642610398556
loss() = 132.79143051306212
loss() = 132.083488106676
loss() = 131.36573282714795
loss() = 130.64103423271692
loss() = 129.91200421447337
loss() = 129.1810050867145
loss() = 128.45015877021888
loss() = 127.7213568311239
loss() = 126.99627117410517
[ Info: Epoch 8
loss() = 126.27636521318334
loss() = 125.56290539059276
loss() = 124.85697291209996
loss() = 124.15947557057872
loss() = 123.47115959116437
loss() = 122.79262141224218
loss() = 122.12431933051133
loss() = 121.46658495126803
loss() = 120.81963439149322
loss() = 120.18357918987661
[ Info: Epoch 9
loss() = 119.55843688398625
loss() = 118.94414121948626
loss() = 118.34055196128998
loss() = 117.74746428028647
loss() = 117.16461769354586
loss() = 116.59170453920123
loss() = 116.02837797087766
loss() = 115.47425945985272
loss() = 114.92894579626136
loss() = 114.39201558398054
[ Info: Epoch 10
loss() = 113.86303522662121
loss() = 113.34156440578403
loss() = 112.82716105568399
loss() = 112.31938583775006
loss() = 111.81780612846966
loss() = 111.32199952993429
loss() = 110.83155691801998
loss() = 110.34608504489371
loss() = 109.86520871472962
loss() = 109.38857255330907
Test Summary: | Pass  Total
Distributed   |    1      1
Starting tests
Going to start!
Finally!
[ Info: Test some gradients
[ Info: Test some fast layers
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:102
[ Info: Test some adjoints
[ Info: Test Tracker
[ Info: Test non-ODEs
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:246
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:262
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:274
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:287
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/gT1qc/test/neural_de.jl:299
Test Summary:   | Pass  Broken  Total
Neural DE Tests |   87      11     98
Test Summary:             | Pass  Total
Augmented Neural DE Tests |    8      8
┌ Warning: CUDA unavailable, not loading CUDA support
└ @ ScatterNNlib ~/.julia/packages/ScatterNNlib/aSacJ/src/ScatterNNlib.jl:47
Test Summary:   | Pass  Total
Neural Graph DE |    1      1
129.8997f0210.07533f081.95683f0158.73064f0156.46371f0132.30234f0119.45451f0107.90214f095.78407f096.14305f0132.25557f0229.81891f0360.44974f0390.36902f0325.55307f0260.98785f0242.21829f0261.18365f0306.6384f0385.49548f0549.24524f0983.42706f01974.7686f02242.1682f04013.6707f09934.283f016680.377f023952.053f029985.475f041168.758f061328.9f085880.85f0112089.9f0141257.16f0172350.28f0217449.06f0273015.5f0328579.9f0212083.9f0192505.94f0204237.98f0214117.23f0139440.56f0140667.19f0188401.2f0252464.48f0271197.34f0276351.8f0300427.28f0332461.1f0366557.1f0378651.16f0465458.44f0502508.66f0552876.2f0616764.8f0678143.5f0748869.5f0801292.56f0887170.94f0956098.6f01.03101625f61.1044386f61.1820442f61.2822601f61.406944f61.0529469f61.3400081f61.3419161f61.394689f61.374162f61.3380948f61.5708422f61.6347346f61.6834291f61.7353309f61.7860069f61.8376895f61.892108f61.9488228f62.0087501f62.0704674f62.1377192f62.208199f62.2827542f62.3586888f62.4339062f62.4986205f62.5994128f62.67899f62.7593362f62.7536682f62.868695f62.8974155f62.9374055f62.9841052f63.0394605f63.1015322f63.1721665f63.249513f63.3343552f63.4252912f63.5252112f63.629776f63.7430792f63.8609995f63.9845215f64.1207985f64.2594315f64.4055985f64.558002f64.7166905f64.883452f65.0537955f65.228091f65.4101195f65.574698f62.5038918f65.5997225f65.4476905f65.3274255f65.303561f65.9392695f64.9763585f62.0632546f61.6477411f61.9938291f62.0875761f61.7919714f61.9056579f61.9234094f61.9268389f61.9274474f61.9289791f61.9323955f61.9350384f61.9360778f61.9448956f61.9521455f61.9580059f61.9627836f61.9668662f61.9707434f61.9742512f61.9775884f61.9806775f61.9836469f61.986401f61.9891081f61.9916218f61.9940846f61.9964356f61.9987151f62.0008368f62.0031221f62.0052629f62.0074629f62.0099484f62.0136508f62.0254259f62.0415252f62.0450815f62.0467311f62.0481382f62.0496076f62.0512376f62.05279f62.0542352f62.0559331f62.0575365f62.059027f62.0607974f62.0624415f62.064006f62.0658138f62.0673894f62.0692105f62.0709516f62.0726518f62.0743259f62.0760531f62.0779518f62.0796821f62.0813921f62.0832829f62.0849869f62.0869045f62.0885902f62.0904939f62.0922639f62.0941854f62.0959364f62.0978875f62.099736f62.1015855f62.1034442f62.1053255f62.107194f62.109034f62.1109228f62.112745f6Test Summary: |
Hybrid DE     | No tests
0.31730079542960450.085377496798027140.029563457354552260.000153727100263548784.7447024958849056e-54.74446404053311e-54.743986921510536e-54.743845296049102e-54.741257678921693e-54.741257678921693e-5Test Summary:       | Pass  Total
Neural ODE MM Tests |    1      1
Test Summary:         | Pass  Total
Fast Neural ODE Tests |    6      6
l = 3.804191949268801
l = 2.964870044307826
l = 2.334356946011484
l = 1.9056403827621446
l = 1.7036054944745438
l = 1.601211286808692
l = 1.5285178754125794
l = 1.43941022808103
l = 1.3029154893135841
l = 1.1585096151636811
l = 1.0334461669614186
l = 0.9780279637993374
l = 0.9859734920627521
l = 0.9921297786939554
l = 0.959517195212159
l = 0.8984792241444747
l = 0.8441184798738244
l = 0.7880819388561178
l = 0.7185851995195948
l = 0.6404855962780651
l = 0.5929325310940651
l = 0.5480014945772734
l = 0.5220751895026084
l = 0.4931563676553094
l = 0.484657367397377
l = 0.46775343821637694
l = 0.44566614635579244
l = 0.4205007914690381
l = 0.3944347757164278
l = 0.3783273089710742
l = 0.35745209958363633
l = 0.33376840402944963
l = 0.2948255356895644
l = 0.2857673289208281
l = 0.29430717663285766
l = 0.29068261183814853
l = 0.2777699344916269
l = 0.26782694691978026
l = 0.2549439159815828
l = 0.2634253699899782
l = 0.24295970768722674
l = 0.24809994994898452
l = 0.23349660748586698
l = 0.21070557767674478
l = 0.223473035358286
l = 0.20852588075955195
l = 0.2093714666637269
l = 0.1983357511037865
l = 0.1977874307580417
l = 0.18739049186489057
l = 0.17468704406601712
l = 0.19080392534553706
l = 0.15975350563394886
l = 0.17481074331350496
l = 0.16312931477675388
l = 0.15257631460312554
l = 0.14817812384423196
l = 0.14790179084484315
l = 0.14395823408787406
l = 0.138299261044687
l = 0.13215637116482906
l = 0.13337674955356302
l = 0.13533342674110807
l = 0.12494804506513493
l = 0.12325308140926189
l = 0.12359269685605019
l = 0.1092119596665832
l = 0.1296241884325181
l = 0.11016192960172685
l = 0.11354680936395409
l = 0.1173288279893677
l = 0.10572342441724551
l = 0.12282449413089935
l = 0.09220013737420216
l = 0.1510288078380408
l = 0.12060843657860457
l = 0.12342957547205488
l = 0.11380442512066843
l = 0.11318198730596014
l = 0.12108208095467601
l = 0.08096322295291145
l = 0.12724736811223022
l = 0.081154144383
l = 0.11599815756981265
l = 0.0803888057662665
l = 0.12931799749509257
l = 0.0914421179726423
l = 0.137527447854724
l = 0.13556904054901442
l = 0.08936500505560974
l = 0.13527214457557152
l = 0.10424213978269681
l = 0.10943570251158373
l = 0.12565468464423254
l = 0.07612444622040807
l = 0.10427881514684069
l = 0.0760361790757943
l = 0.09379125351388513
l = 0.06475441886758317
l = 0.09436839711714164
l = 0.06475441886758317
l = 0.08116245416579322
l = 0.08330305738218798
l = 0.07319756570230841
l = 0.06307844631969736
l = 0.06280592360562559
l = 0.0602764089971633
l = 0.05707519374029058
l = 0.05888731765218362
l = 0.056955879897725464
l = 0.0556450579923621
l = 0.05524701721564794
l = 0.0543953835808055
l = 0.05268824281613557
l = 0.05173570877375254
l = 0.052700281821193545
l = 0.052271783889704486
l = 0.05040541231333808
l = 0.049666144312884956
l = 0.050591065301502744
l = 0.05076939089471618
l = 0.04929587739635606
l = 0.04857562211080525
l = 0.04894125609399586
l = 0.04895523716196323
l = 0.04799154371652352
l = 0.0474978309810709
l = 0.04771571547193335
l = 0.04752912973679184
l = 0.04712404205742279
l = 0.04668908888955324
l = 0.0467563218803124
l = 0.04656837069051259
l = 0.04613331002770285
l = 0.04591261508857578
l = 0.0457825875517969
l = 0.04563666189221729
l = 0.045322811135697215
l = 0.04517102344772035
l = 0.04509000149054331
l = 0.04478781287340903
l = 0.044518048893622694
l = 0.04443487156830008
l = 0.044331972767897174
l = 0.043908561880657
l = 0.043788121691456995
l = 0.04376907508878448
l = 0.04343423754620949
l = 0.04328827196487521
l = 0.04320976153959829
l = 0.04293139084394448
l = 0.042738330636236395
l = 0.04265475035444242
l = 0.042453015393036614
l = 0.04225947797143202
l = 0.042124257164698545
l = 0.041936220019103694
l = 0.041785912713333075
l = 0.04161209075603163
l = 0.04143327590985692
l = 0.04128890654567668
l = 0.04112569080923389
l = 0.04095439477270565
l = 0.04079878506174109
l = 0.04064816991774688
l = 0.04048464797483333
l = 0.04031959155710629
l = 0.04015474877419916
l = 0.04002024810795115
l = 0.03986306513202332
l = 0.039685393710115785
l = 0.03954821312727225
l = 0.039380890138714784
l = 0.03922167299746959
l = 0.03907656995902188
l = 0.03892291129176101
l = 0.03877812790685314
l = 0.03861011097100768
l = 0.03845577361721414
l = 0.038305062945398344
l = 0.03815740622688879
l = 0.037996906739049485
l = 0.037849372040977436
l = 0.03770016284262323
l = 0.03754474680576061
l = 0.037398357158375285
l = 0.03724181125108589
l = 0.03708959481347705
l = 0.036940809986167424
l = 0.0368044388586027
l = 0.03664920017500211
l = 0.03651654701652107
l = 0.036358592655278056
l = 0.03620976779228152
l = 0.03606802774897964
l = 0.035920398004237054
l = 0.03577294559858745
l = 0.035642058751090076
l = 0.03547815602628852
l = 0.035343036486505795
l = 0.03520694231673149
l = 0.03520694231673149
l = 0.03504934103335185
l = 0.03503839815505237
l = 0.03503614651128082
l = 0.03490562203906499
l = 0.034719947276078994
l = 0.03447842592867473
l = 0.034153953437245074
l = 0.03368351400755952
l = 0.033292847470855556
l = 0.03319534166734441
l = 0.032923522302896184
l = 0.032407547059324096
l = 0.03219758260011921
l = 0.031561742760948315
l = 0.03090570214355608
l = 0.030478859012541798
l = 0.03004288729072722
l = 0.029365866936579853
l = 0.028434033573771732
l = 0.02794691043901944
l = 0.02761845249072087
l = 0.026898230241414706
l = 0.026307666432732583
l = 0.02534227340117647
l = 0.02466690973164144
l = 0.023799932820794928
l = 0.023054561344825864
l = 0.022496387844091838
l = 0.021565616284658227
l = 0.020767643487035047
l = 0.020261373748583417
l = 0.019293216515507507
l = 0.01869797238160256
l = 0.01820617167190421
l = 0.01755415301607306
l = 0.016900726844402247
l = 0.016368740054889842
l = 0.01570714989669331
l = 0.015186022400796823
l = 0.01477457692082888
l = 0.014347679915760437
l = 0.01387793778800732
l = 0.013420169443181537
l = 0.013042214575305385
l = 0.012766085793908153
l = 0.012554979580844082
l = 0.012385642225534664
l = 0.012249621092289142
l = 0.012007206130648406
l = 0.011781947438051384
l = 0.011444248929800373
l = 0.011268130715202782
l = 0.011088430123624218
l = 0.010935833725494589
l = 0.010769719056166381
l = 0.010560608899436172
l = 0.01042206117984735
l = 0.010314883233871395
l = 0.010189223104561988
l = 0.010033278704064867
l = 0.009850324879879893
l = 0.009589738790036478
l = 0.00939895055116192
l = 0.009249134848856673
l = 0.009134809796795994
l = 0.008987026371512738
l = 0.008794424029116933
l = 0.008596036173531576
l = 0.008367040590980132
l = 0.00816014806013133
l = 0.007912681144356544
l = 0.007689538555280505
l = 0.007393676784984094
l = 0.007265764062556443
l = 0.007140998399080015
l = 0.007051043181784156
l = 0.006952385988227701
l = 0.006823817060389133
l = 0.006724406915701261
l = 0.006617667760240118
l = 0.006532550563298049
l = 0.006475700776435349
l = 0.006443270587758365
l = 0.006389599065693258
l = 0.006312907659701263
l = 0.006233282449476534
l = 0.006153079962900668
l = 0.006091960158793944
l = 0.0059926073542140666
l = 0.0058876142115649385
l = 0.005809463405625761
l = 0.005764578793569937
l = 0.005739557171316172
l = 0.005724523794927533
l = 0.005709870119079321
l = 0.0056846065985410605
l = 0.005655238134862829
l = 0.005629978574241409
l = 0.00561151381443302
l = 0.005587270479620326
l = 0.005569441833522675
l = 0.005541232296830082
l = 0.0055208319281835365
l = 0.005467619316563692
l = 0.005415256684966075
l = 0.005367464274001903
l = 0.0053143430332871335
l = 0.005277077044241972
l = 0.005245916461011422
l = 0.005203793554258949
l = 0.005156749237045922
l = 0.005108562918958316
l = 0.005052848393748385
l = 0.004987962562806439
l = 0.0049232603881465
l = 0.004852592115823282
l = 0.00473617035553987
l = 0.004562028010923779
l = 0.0043826167397210585
l = 0.004304780623537587
l = 0.004256942954876317
l = 0.004201223907324321
l = 0.004160563659767179
l = 0.00412146948260867
l = 0.004087180636925892
l = 0.004044031555801874
l = 0.0039967008482852465
l = 0.003971056847098548
l = 0.00395741479279458
l = 0.003938238423991926
l = 0.003913515965300629
l = 0.0038919939347582437
l = 0.003871739357879747
l = 0.003837037534834471
l = 0.0038076633708213447
l = 0.0037814078506198095
l = 0.003756679843182473
l = 0.0037214360668617924
l = 0.0036781541800133552
l = 0.0036206599445645525
l = 0.0035671351985562807
l = 0.0035192021411539877
l = 0.003489040081379315
l = 0.003468526282851289
l = 0.003442555363585292
l = 0.0034155032663378414
l = 0.0033833643143423653
l = 0.00334266894794378
l = 0.003314139259369779
l = 0.0032967114837038334
l = 0.0032736826932909537
l = 0.0032440605663941672
l = 0.003214276940775777
l = 0.0031793248235814477
l = 0.003143618212012325
l = 0.003117693223872586
l = 0.0031013910287984413
l = 0.0030879126460566357
l = 0.0030680628829209042
l = 0.0030472538850001563
l = 0.0030259686184711267
l = 0.0029999390861665223
l = 0.00296278828359029
l = 0.0029162502868745385
l = 0.0028277955070200762
l = 0.0027720227593312553
l = 0.0027090392022328276
l = 0.0026534891054829867
l = 0.002603410847843109
l = 0.0025593503009021194
l = 0.00252747432287041
l = 0.0024939111017448674
l = 0.002459158823670059
l = 0.0024324888340480338
l = 0.002413971555160714
l = 0.0024055935071952716
l = 0.00239055520242175
l = 0.0023707136464061725
l = 0.002355334053872132
l = 0.002344772617690476
l = 0.0023343342462543893
l = 0.002315556942860438
l = 0.002290303780822722
l = 0.0022481800760531983
l = 0.0022043621323686057
l = 0.0021449607821011174
l = 0.0020978476401541032
l = 0.0020560304752757605
l = 0.002030215776274663
l = 0.0019920439106725957
l = 0.0019498848470483902
l = 0.0018960344735351295
l = 0.0018696717433187772
l = 0.0018504977329747505
l = 0.001836233140423252
l = 0.0018260582171219259
l = 0.0018110930976243753
l = 0.0017964540311697823
l = 0.0017828564460608703
l = 0.0017703668137556434
l = 0.001755742513901506
l = 3.665137878870346
l = 2.6562978462376057
l = 2.010087018810974
l = 1.7489946449390825
l = 1.6205165585213182
l = 1.535772195291694
l = 1.4156731835636154
l = 1.3096859267343777
l = 1.1374652680209194
l = 0.944607053553677
l = 0.885484952431525
l = 0.9539706744734379
l = 0.9814709807856037
l = 0.918873678662298
l = 0.7903826442465273
l = 0.6672548706839359
l = 0.6338046015261882
l = 0.6139558360073025
l = 0.5850395072536683
l = 0.5173513095369354
l = 0.4305899307789866
l = 0.3919688529816061
l = 0.4321645701272868
l = 0.448526032731591
l = 0.45372664585721034
l = 0.4374077543808458
l = 0.3714879105313703
l = 0.31783711423819555
l = 0.30544016709650945
l = 0.26085915892780237
l = 0.270080923494499
l = 0.27631542063157044
l = 0.25576903326285677
l = 0.25123335647689704
l = 0.23130256986283906
l = 0.22659958202316785
l = 0.20800552207811554
l = 0.20047065309603437
l = 0.19754189448780324
l = 0.2132955174079293
l = 0.20214032411186417
l = 0.1883620085517425
l = 0.1727018392110376
l = 0.16258953631386913
l = 0.15038670184068725
l = 0.16501269313031272
l = 0.1535825788371585
l = 0.15141675478976824
l = 0.15028492111090583
l = 0.1335790279655182
l = 0.12332781294909413
l = 0.135593975422892
l = 0.1414143728206373
l = 0.12286586207896268
l = 0.12202068735826156
l = 0.11582161245902979
l = 0.11878966718979034
l = 0.11964503360493582
l = 0.10320376020292395
l = 0.1170222850374955
l = 0.10596209589801378
l = 0.09972350062562317
l = 0.10262101548900968
l = 0.09961317808402409
l = 0.0863485243575292
l = 0.09935302427539404
l = 0.09235446182384276
l = 0.08992525056305886
l = 0.10260065837269848
l = 0.08737088122567897
l = 0.08978345611269152
l = 0.09592581380224675
l = 0.08819011540115841
l = 0.07115944160279425
l = 0.10977162038484904
l = 0.10108802552231785
l = 0.08331711935445346
l = 0.11282184166028734
l = 0.07819737580941014
l = 0.12782577269323198
l = 0.08220436940945222
l = 0.10038066148676462
l = 0.0930684761263781
l = 0.08555153236704358
l = 0.10617581991720497
l = 0.09649041277181955
l = 0.10332151344770688
l = 0.07844953791272037
l = 0.08211174895586569
l = 0.09163813606491346
l = 0.08778404040280544
l = 0.07963006777461218
l = 0.08968709601518973
l = 0.07102992211831599
l = 0.07982393893731947
l = 0.08648604710027467
l = 0.07912842743565635
l = 0.07200116966416938
l = 0.08401188253085157
l = 0.07195762903919085
l = 0.07102992211831599
l = 0.08914090787588769
l = 0.08258033840441774
l = 0.0702459740497931
l = 0.06304210310582059
l = 0.06332699057941968
l = 0.06100038956616504
l = 0.05692163474961141
l = 0.057959309446370996
l = 0.05538965794352196
l = 0.0545131272230596
l = 0.05035963083039997
l = 0.05322311507540278
l = 0.05398201268450597
l = 0.05012104012612974
l = 0.04775743008000935
l = 0.04981689344541387
l = 0.05043439758550993
l = 0.04915177496406342
l = 0.04766420630441753
l = 0.046526311677686255
l = 0.04717885336336879
l = 0.04680496296830284
l = 0.046847787717970754
l = 0.04649701111644449
l = 0.0451251074424718
l = 0.04543255223916848
l = 0.045065906323953575
l = 0.04521151015105676
l = 0.04517718356709181
l = 0.045057560462760785
l = 0.04441244363098987
l = 0.04361301943124391
l = 0.04448364693435181
l = 0.04394505055644874
l = 0.04356380400064698
l = 0.04358394358981507
l = 0.04319102237185765
l = 0.043101455695518175
l = 0.043216778525827046
l = 0.042748660118329944
l = 0.04256396439152974
l = 0.04236664903572708
l = 0.04246432001052996
l = 0.042223548032142816
l = 0.04211024665575851
l = 0.04200697696388437
l = 0.04181010879325438
l = 0.04168591560726504
l = 0.04169490421752102
l = 0.04154660132551174
l = 0.041325731826855554
l = 0.04103154975174539
l = 0.04105339101040977
l = 0.04095947354920868
l = 0.040782650082615675
l = 0.04078002702143254
l = 0.04067410378287955
l = 0.04048722800476957
l = 0.04024425327165332
l = 0.04023902295879186
l = 0.04036029177043378
l = 0.040203075664572326
l = 0.039922969845266845
l = 0.03966057288372392
l = 0.03961469717714326
l = 0.03959749245759796
l = 0.03959775414974416
l = 0.03936638648022783
l = 0.03911884125861
l = 0.03900827074273777
l = 0.03895876915807206
l = 0.03899943347940861
l = 0.03898445979600792
l = 0.03870362262858769
l = 0.03851273387349265
l = 0.03831850969649752
l = 0.03815992467416004
l = 0.038043251970945824
l = 0.03792566346970467
l = 0.03787877429366741
l = 0.03786062965595311
l = 0.03794603444400769
l = 0.03775814201317651
l = 0.037651930123424805
l = 0.03735630478392743
l = 0.03718801526475283
l = 0.03701665376968145
l = 0.03692662427586239
l = 0.0368233140791468
l = 0.03682454265018329
l = 0.03673318442725159
l = 0.036853321331638725
l = 0.03670282646689364
l = 0.03672394562251129
l = 0.0363231094146681
l = 0.036071851455324544
l = 0.03584881222362753
l = 0.03567470718319806
l = 0.03556271722841151
l = 0.03544511718769417
l = 0.03544511718769417
l = 0.035350454209046485
l = 0.035325392055261244
l = 0.03531473960092808
l = 0.03528001461246024
l = 0.03524464852525761
l = 0.03508277000032058
l = 0.034599512174243
l = 0.03423560885756127
l = 0.034011320919684625
l = 0.03371607679248085
l = 0.03349117067154197
l = 0.03212590305202184
l = 0.030232913487688393
l = 0.028368395851053132
l = 0.026766946428055563
l = 0.024875609781353763
l = 0.023589406050137467
l = 0.022684270009692732
l = 0.02174537890251747
l = 0.021340905267855797
l = 0.021170255321482773
l = 0.020798351465918487
l = 0.02035633306574903
l = 0.02003628884764018
l = 0.01973905325990628
l = 0.019392628136115465
l = 0.018694423132358612
l = 0.01805920326449908
l = 0.017485582913526747
l = 0.016919933473353466
l = 0.016529038701402463
l = 0.01632717385131386
l = 0.016186962445061712
l = 0.01599741745706248
l = 0.015850823236718527
l = 0.015539105104832954
l = 0.015261491732744083
l = 0.014876405188338052
l = 0.014650591848631612
l = 0.014483327700336533
l = 0.014304314038303824
l = 0.014022715901064227
l = 0.01382159853612986
l = 0.013588348919850718
l = 0.01320229478875644
l = 0.012756732874981589
l = 0.012250529651162195
l = 0.011863906103697525
l = 0.011437585307788031
l = 0.011250042462820969
l = 0.01111186853000764
l = 0.010912517937858024
l = 0.010471601099137886
l = 0.010114183845967422
l = 0.009897731205215213
l = 0.009762145718635437
l = 0.009563338021623003
l = 0.00948851317832952
l = 0.009384572216944567
l = 0.009314902684969349
l = 0.009187850308990003
l = 0.008944495803377698
l = 0.008583817698118572
l = 0.008205473431966039
l = 0.007925737686632621
l = 0.0076942516963643655
l = 0.007530636667805914
l = 0.007362912759815542
l = 0.007124459249960116
l = 0.006885080394058192
l = 0.00645821480790563
l = 0.006035737069832188
l = 0.005869861057273327
l = 0.005663660469544677
l = 0.005483666334119487
l = 0.0053842328849249416
l = 0.005292103628309296
l = 0.0051964487309845275
l = 0.005125036343846786
l = 0.00509049028121562
l = 0.005051906426787673
l = 0.005022164554760491
l = 0.004997572696212128
l = 0.004961460517791962
l = 0.004933724502602267
l = 0.004901553527798382
l = 0.004859864059922579
l = 0.004807109597711444
l = 0.004685695887119117
l = 0.00450012590256489
l = 0.004376941648466726
l = 0.004331429273639222
l = 0.004279403648846556
l = 0.0042383934388886555
l = 0.00420561655698062
l = 0.004140920286971362
l = 0.004091647790953322
l = 0.004070034687898396
l = 0.00405492555811101
l = 0.004046763236389176
l = 0.004039940840351058
l = 0.004033041460191938
l = 0.004026827552680273
l = 0.004017344365102528
l = 0.004005041258189268
l = 0.003992607859204021
l = 0.003980935557142512
l = 0.003928579013011578
l = 0.003863322754136619
l = 0.003798598723635401
l = 0.0037571242218918076
l = 0.003737293541263734
l = 0.0037288082106395307
l = 0.003721437138587192
l = 0.00371718004653076
l = 0.0037137970978541953
l = 0.0037117934155549053
l = 0.0037048034898144854
l = 0.003687239753583607
l = 0.003654233672072692
l = 0.0036154241689396833
l = 0.0035890422034096435
l = 0.0035659979782487598
l = 0.0035507049832910038
l = 0.0035398418220028414
l = 0.003533038285021237
l = 0.003526176686966239
l = 0.003515007446629827
l = 0.0035015899479500534
l = 0.003482832946593208
l = 0.0034439913511038773
l = 0.003403359373274512
l = 0.003331513832046302
l = 0.0032651646156039225
l = 0.0032174353836280263
l = 0.0031573083383658316
l = 0.003128603769138585
l = 0.0031136214397205226
l = 0.0031016759747354286
l = 0.0030854172078734416
l = 0.0030722262160582724
l = 0.003063732269248828
l = 0.003047387164604693
l = 0.0030247629770750748
l = 0.0029902228825433724
l = 0.002916778537798124
l = 0.0027819845180764463
l = 0.002604559224081079
l = 0.0024110342744616464
l = 0.002251105077182514
l = 0.0021481951956880325
l = 0.0020856648921866996
l = 0.0020571081897267675
l = 0.0020468067105786697
l = 0.002034217675956191
l = 0.002021839045538199
l = 0.0020087296089402193
l = 0.0019788035115677633
l = 0.0019448924406613541
l = 0.0018915108036944947
l = 0.0018353614672153374
l = 0.0018048949780320384
l = 0.0017845386793694993
l = 0.0017670129524107593
l = 0.0017586617436176392
l = 0.0017445228388697843
l = 0.001719350237128485
l = 0.0016812827601750705
l = 0.0016083651351974483
l = 0.001512351269911224
l = 0.0014418488512291682
l = 0.0013814034695456162
l = 0.0013586487245961542
l = 0.0013437926582508754
l = 0.0013349109596093648
l = 0.0013282711246845335
l = 0.0013248909328977233
l = 0.0013175172016286576
l = 0.0013098881671523884
l = 0.0012951208061682054
l = 0.001267868575021955
l = 0.0012298535020297304
l = 0.0012061951532534665
l = 0.001193859813651877
l = 0.0011878207592553687
l = 0.0011839255644432206
l = 0.0011806016187603748
l = 0.0011766756935862857
l = 0.0011735073432017214
l = 0.0011636589867941726
l = 0.0011400497792686064
l = 0.0011107292333753071
l = 0.00106351933768256
l = 0.0010241076330556392
l = 0.0010027808602358938
l = 0.0009940408917558886
l = 0.0009905736607732682
l = 0.0009807276283702317
l = 0.0009763644475008411
l = 0.0009699855742713463
l = 0.0009646507179954544
Test Summary:        | Pass  Total
Tensor Product Layer |    2      2
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
l = 0.8407523985803061
Spline Layer: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:43
  Expression: run_test(f, layer, 0.05)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:43
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
l = 0.777234585920141
Spline Layer: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:49
  Expression: run_test(f, layer, 0.05)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:49
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
l = 1.000905312664025
Spline Layer: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:55
  Expression: run_test(f, layer, 0.05)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:55
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
l = 1.3823456488857362
Spline Layer: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:60
  Expression: run_test(f, layer, 0.05)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/spline_layer_test.jl:60
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:35
Test Summary: | Fail  Total
Spline Layer  |    4      4
ERROR: LoadError: Some tests did not pass: 0 passed, 4 failed, 0 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/DiffEqFlux/gT1qc/test/runtests.jl:16

PkgEval failed: Package DiffEqFlux errored during testing
Stacktrace:
 [1] pkgerror(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1578
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:327
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:314
 [5] #test#61 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [7] #test#60 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [11] top-level scope at none:22
 [12] eval at ./boot.jl:331 [inlined]
 [13] eval(::Expr) at ./client.jl:467
 [14] top-level scope at none:1
 [15] eval(::Module, ::Any) at ./boot.jl:331
 [16] exec_options(::Base.JLOptions) at ./client.jl:272
 [17] _start() at ./client.jl:506


################################################################################
# PkgEval teardown: 2021-01-11T16:03:56.444
CPU usage: 2304.02s (2238.37s user, 67.92s kernel)

Network usage:
- eth0: 290.872 MiB received, 3.351 MiB sent

Raw statistics: {"blkio_stats":{"io_queue_recursive":[],"io_serviced_recursive":[{"major":259,"value":145,"op":"Read","minor":0},{"major":259,"value":60,"op":"Write","minor":0},{"major":259,"value":166,"op":"Sync","minor":0},{"major":259,"value":39,"op":"Async","minor":0},{"major":259,"value":0,"op":"Discard","minor":0},{"major":259,"value":205,"op":"Total","minor":0}],"io_time_recursive":[],"sectors_recursive":[],"io_service_bytes_recursive":[{"major":259,"value":6602752,"op":"Read","minor":0},{"major":259,"value":356352,"op":"Write","minor":0},{"major":259,"value":6717440,"op":"Sync","minor":0},{"major":259,"value":241664,"op":"Async","minor":0},{"major":259,"value":0,"op":"Discard","minor":0},{"major":259,"value":6959104,"op":"Total","minor":0}],"io_wait_time_recursive":[],"io_merged_recursive":[],"io_service_time_recursive":[]},"memory_stats":{"usage":2228322304,"max_usage":9089003520,"stats":{"hierarchical_memory_limit":9223372036854771712,"pgmajfault":33,"total_rss":299593728,"inactive_file":17121280,"inactive_anon":1268006912,"total_pgfault":46492809,"total_cache":1862279168,"total_rss_huge":41943040,"total_active_anon":868007936,"total_pgmajfault":33,"pgpgin":39697185,"rss_huge":41943040,"mapped_file":0,"total_inactive_file":17121280,"dirty":135168,"total_unevictable":0,"total_dirty":135168,"writeback":0,"pgpgout":39180215,"hierarchical_memsw_limit":0,"cache":1862279168,"active_file":8515584,"pgfault":46492809,"total_mapped_file":0,"total_pgpgout":39180215,"total_writeback":0,"rss":299593728,"unevictable":0,"total_inactive_anon":1268006912,"total_pgpgin":39697185,"active_anon":868007936,"total_active_file":8515584},"limit":67467247616},"name":"/DiffEqFlux-DfvcqE5d","networks":{"eth0":{"rx_bytes":305001642,"rx_packets":62424,"tx_packets":40847,"tx_bytes":3514097,"tx_dropped":0,"rx_dropped":0,"tx_errors":0,"rx_errors":0}},"id":"fb13a64f9747dc1b15083ddba961e3780e7009569c4c13e8f3c70d644b725905","preread":"0001-01-01T00:00:00Z","storage_stats":{},"precpu_stats":{"throttling_data":{"throttled_periods":0,"throttled_time":0,"periods":0},"cpu_usage":{"usage_in_usermode":0,"usage_in_kernelmode":0,"total_usage":0}},"pids_stats":{"current":44},"num_procs":0,"cpu_stats":{"throttling_data":{"throttled_periods":0,"throttled_time":0,"periods":0},"system_cpu_usage":7263317175808967,"online_cpus":64,"cpu_usage":{"usage_in_usermode":2238370000000,"usage_in_kernelmode":67920000000,"total_usage":2304024947973,"percpu_usage":[0,0,2304024939959,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8014,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}},"read":"2021-01-11T16:03:56.446631242Z"}